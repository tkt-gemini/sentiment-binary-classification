"""
Preprocessing Module
Module ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu cho d·ª± √°n ph√¢n t√≠ch c·∫£m x√∫c ti·∫øng Vi·ªát
"""

import pandas as pd
import numpy as np
import re
import unicodedata
import string
from typing import List, Set, Optional

try:
    from underthesea import word_tokenize
    USE_UNDERTHESEA = True
except ImportError:
    USE_UNDERTHESEA = False
    print("Warning: underthesea kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t. S·ª≠ d·ª•ng tokenizer ƒë∆°n gi·∫£n.")


def load_stopwords(filepath: str = 'vietnamese-stopwords.txt') -> Set[str]:
    """
    T·∫£i danh s√°ch stopwords t·ª´ file
    
    Parameters:
    -----------
    filepath : str
        ƒê∆∞·ªùng d·∫´n t·ªõi file stopwords
    
    Returns:
    --------
    Set[str] : T·∫≠p h·ª£p c√°c stopwords
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            stopwords = set(line.strip() for line in f if line.strip())
        print(f"‚úÖ ƒê√£ t·∫£i {len(stopwords)} stopwords t·ª´ {filepath}")
        return stopwords
    except FileNotFoundError:
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file {filepath}. S·ª≠ d·ª•ng t·∫≠p r·ªóng.")
        return set()


def remove_punctuation(text: str) -> str:
    """X√≥a d·∫•u c√¢u"""
    return text.translate(str.maketrans('', '', string.punctuation))


def normalize_unicode(text: str) -> str:
    """Chu·∫©n h√≥a Unicode ti·∫øng Vi·ªát"""
    return unicodedata.normalize('NFC', text)


def remove_duplicate_characters(text: str) -> str:
    """X√≥a c√°c k√Ω t·ª± l·∫∑p li√™n ti·∫øp (vd: 'haaay' -> 'hay')"""
    return re.sub(r'(.)\1+', r'\1', text)


def remove_emoji(text: str) -> str:
    """X√≥a emoji kh·ªèi vƒÉn b·∫£n"""
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)


def remove_urls(text: str) -> str:
    """X√≥a URLs kh·ªèi vƒÉn b·∫£n"""
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)


def remove_stopwords(text: str, stopwords: Set[str]) -> str:
    """Lo·∫°i b·ªè stopwords"""
    tokens = text.split()
    clean_tokens = [word for word in tokens if word not in stopwords]
    return ' '.join(clean_tokens)


def preprocess_text(text: str, stopwords: Optional[Set[str]] = None) -> str:
    """
    H√†m ti·ªÅn x·ª≠ l√Ω ch√≠nh cho vƒÉn b·∫£n ti·∫øng Vi·ªát.
    
    Pipeline:
    1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    2. Chu·∫©n h√≥a Unicode
    3. X√≥a URLs v√† emoji
    4. X√≥a k√Ω t·ª± l·∫∑p
    5. X√≥a d·∫•u c√¢u
    6. Tokenize (t√°ch t·ª´ ti·∫øng Vi·ªát)
    7. Lo·∫°i b·ªè stopwords
    
    Parameters:
    -----------
    text : str
        VƒÉn b·∫£n c·∫ßn x·ª≠ l√Ω
    stopwords : Set[str], optional
        T·∫≠p h·ª£p c√°c stopwords c·∫ßn lo·∫°i b·ªè
    
    Returns:
    --------
    str : VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
    """
    if not isinstance(text, str) or len(text.strip()) == 0:
        return ""
    
    # 1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng v√† lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    text = text.lower().strip()
    
    # 2. Chu·∫©n h√≥a Unicode
    text = normalize_unicode(text)
    
    # 3. X√≥a URLs v√† emoji
    text = remove_urls(text)
    text = remove_emoji(text)
    
    # 4. X√≥a k√Ω t·ª± l·∫∑p
    text = remove_duplicate_characters(text)
    
    # 5. X√≥a d·∫•u c√¢u
    text = remove_punctuation(text)
    
    # 6. Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    text = ' '.join(text.split())
    
    # 7. Tokenize (T√°ch t·ª´ ti·∫øng Vi·ªát)
    if USE_UNDERTHESEA:
        text = word_tokenize(text, format="text")
    
    # 8. Lo·∫°i b·ªè stopwords
    if stopwords is not None:
        text = remove_stopwords(text, stopwords)
    
    return text.strip()


def preprocess_dataframe(df: pd.DataFrame, 
                        text_column: str = 'sentence',
                        stopwords: Optional[Set[str]] = None) -> pd.DataFrame:
    """
    Ti·ªÅn x·ª≠ l√Ω to√†n b·ªô DataFrame
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame ch·ª©a d·ªØ li·ªáu
    text_column : str
        T√™n c·ªôt ch·ª©a vƒÉn b·∫£n c·∫ßn x·ª≠ l√Ω
    stopwords : Set[str], optional
        T·∫≠p h·ª£p c√°c stopwords
    
    Returns:
    --------
    pd.DataFrame : DataFrame v·ªõi c·ªôt m·ªõi ch·ª©a vƒÉn b·∫£n ƒë√£ x·ª≠ l√Ω
    """
    df = df.copy()
    df['sentence_processed'] = df[text_column].apply(
        lambda x: preprocess_text(x, stopwords)
    )
    
    # Lo·∫°i b·ªè c√°c d√≤ng c√≥ vƒÉn b·∫£n r·ªóng sau khi x·ª≠ l√Ω
    df = df[df['sentence_processed'].str.len() > 0]
    df.reset_index(drop=True, inplace=True)
    
    return df


if __name__ == "__main__":
    # Test
    sample_texts = [
        "Th·∫ßy gi·∫£ng b√†i r·∫•t hay v√† d·ªÖ hi·ªÉu!!! üòä",
        "Gi·∫£ng h∆°i bu·ªìn ng·ªß, c·∫ßn c·∫£i thi·ªán th√™m...",
        "C∆° s·ªü v·∫≠t ch·∫•t r·∫•t tuy·ªát v·ªùi!!!!"
    ]
    
    stopwords = load_stopwords()
    
    print("\n" + "="*50)
    print("KI·ªÇM TRA TI·ªÄN X·ª¨ L√ù")
    print("="*50)
    
    for text in sample_texts:
        processed = preprocess_text(text, stopwords)
        print(f"\nG·ªëc: {text}")
        print(f"X·ª≠ l√Ω: {processed}")

