"""
Training Module
Module huáº¥n luyá»‡n mÃ´ hÃ¬nh phÃ¢n tÃ­ch cáº£m xÃºc tiáº¿ng Viá»‡t
"""

import pandas as pd
import numpy as np
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, f1_score, accuracy_score
from preprocess import load_stopwords, preprocess_dataframe
import warnings
warnings.filterwarnings('ignore')


def load_data(data_path: str = '../archive'):
    """
    Load dá»¯ liá»‡u tá»« file JSON
    
    Parameters:
    -----------
    data_path : str
        ÄÆ°á»ng dáº«n tá»›i thÆ° má»¥c chá»©a data
    
    Returns:
    --------
    dict : Dictionary chá»©a train, dev, test DataFrames
    """
    print("ğŸ“‚ Äang táº£i dá»¯ liá»‡u...")
    
    df = {}
    for split in ['train', 'dev', 'test']:
        filepath = f'{data_path}/UIT-VSFC-{split}.json'
        df[split] = pd.read_json(filepath)
        
        # Lá»c chá»‰ láº¥y topic 'lecturer' vÃ  bá» 'neutral'
        df[split] = df[split][
            (df[split]['topic'] == 'lecturer') & 
            (df[split]['sentiment'] != 'neutral')
        ].drop('topic', axis=1)
        
        df[split].reset_index(drop=True, inplace=True)
        print(f"  âœ… {split.capitalize()}: {df[split].shape[0]} máº«u")
    
    return df


def encode_labels(df: dict):
    """
    Encode nhÃ£n sentiment thÃ nh sá»‘
    
    Parameters:
    -----------
    df : dict
        Dictionary chá»©a train, dev, test DataFrames
    
    Returns:
    --------
    LabelEncoder : Encoder Ä‘Ã£ Ä‘Æ°á»£c fit
    """
    print("\nğŸ”¢ Äang encode nhÃ£n...")
    
    label_encoder = LabelEncoder()
    df['train']['sentiment_encoded'] = label_encoder.fit_transform(df['train']['sentiment'])
    df['dev']['sentiment_encoded'] = label_encoder.transform(df['dev']['sentiment'])
    df['test']['sentiment_encoded'] = label_encoder.transform(df['test']['sentiment'])
    
    print(f"  Mapping: negative={label_encoder.transform(['negative'])[0]}, positive={label_encoder.transform(['positive'])[0]}")
    
    return label_encoder


def create_pipeline(model_type='svm'):
    """
    Táº¡o pipeline cho model
    
    Parameters:
    -----------
    model_type : str
        Loáº¡i model: 'svm', 'lr', 'nb'
    
    Returns:
    --------
    Pipeline : Sklearn pipeline
    """
    # Chá»n model
    if model_type == 'svm':
        classifier = LinearSVC(
            C=1.0, 
            class_weight='balanced',
            max_iter=2000,
            random_state=42
        )
    elif model_type == 'lr':
        classifier = LogisticRegression(
            C=1.0,
            class_weight='balanced',
            max_iter=1000,
            random_state=42,
            n_jobs=-1
        )
    elif model_type == 'nb':
        classifier = MultinomialNB(alpha=1.0)
    else:
        raise ValueError(f"Model type '{model_type}' khÃ´ng há»£p lá»‡")
    
    # Táº¡o pipeline
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            min_df=2,
            sublinear_tf=True
        )),
        ('clf', classifier)
    ])
    
    return pipeline


def find_optimal_threshold(model, X_dev, y_dev):
    """
    TÃ¬m ngÆ°á»¡ng tá»‘i Æ°u Ä‘á»ƒ maximize F1-score trÃªn dev set
    
    Parameters:
    -----------
    model : sklearn model
        Model Ä‘Ã£ Ä‘Æ°á»£c train
    X_dev : array-like
        Dev set features
    y_dev : array-like
        Dev set labels
    
    Returns:
    --------
    float : NgÆ°á»¡ng tá»‘i Æ°u
    """
    # Láº¥y decision function hoáº·c probability
    if hasattr(model, 'decision_function'):
        scores = model.decision_function(X_dev)
        # Convert to probability-like scores
        scores = 1 / (1 + np.exp(-scores))
    else:
        scores = model.predict_proba(X_dev)[:, 1]
    
    best_f1 = 0
    best_threshold = 0.5
    
    # Thá»­ cÃ¡c ngÆ°á»¡ng tá»« 0.3 Ä‘áº¿n 0.7
    for threshold in np.arange(0.3, 0.8, 0.01):
        y_pred = (scores >= threshold).astype(int)
        f1 = f1_score(y_dev, y_pred)
        
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    return best_threshold


def evaluate_model(model, X, y, threshold=0.5):
    """
    ÄÃ¡nh giÃ¡ model
    
    Parameters:
    -----------
    model : sklearn model
        Model Ä‘Ã£ Ä‘Æ°á»£c train
    X : array-like
        Features
    y : array-like
        True labels
    threshold : float
        NgÆ°á»¡ng Ä‘á»ƒ classify
    
    Returns:
    --------
    dict : Dictionary chá»©a cÃ¡c metrics
    """
    # Dá»± Ä‘oÃ¡n
    if hasattr(model, 'decision_function'):
        scores = model.decision_function(X)
        scores = 1 / (1 + np.exp(-scores))
        y_pred = (scores >= threshold).astype(int)
    else:
        probs = model.predict_proba(X)[:, 1]
        y_pred = (probs >= threshold).astype(int)
    
    # TÃ­nh metrics
    accuracy = accuracy_score(y, y_pred)
    f1 = f1_score(y, y_pred)
    
    return {
        'accuracy': accuracy,
        'f1_score': f1,
        'predictions': y_pred
    }


def train_and_save_model(data_path='../archive', 
                         output_dir='models',
                         model_type='svm'):
    """
    HÃ m chÃ­nh Ä‘á»ƒ train vÃ  lÆ°u model
    
    Parameters:
    -----------
    data_path : str
        ÄÆ°á»ng dáº«n tá»›i thÆ° má»¥c chá»©a data
    output_dir : str
        ThÆ° má»¥c Ä‘á»ƒ lÆ°u models
    model_type : str
        Loáº¡i model: 'svm', 'lr', 'nb'
    """
    print("="*60)
    print("ğŸš€ Báº®T Äáº¦U TRAINING MODEL")
    print("="*60)
    
    # 1. Load dá»¯ liá»‡u
    df = load_data(data_path)
    
    # 2. Encode labels
    label_encoder = encode_labels(df)
    
    # 3. Tiá»n xá»­ lÃ½
    print("\nğŸ”§ Äang tiá»n xá»­ lÃ½ vÄƒn báº£n...")
    stopwords = load_stopwords(f'{data_path}/vietnamese-stopwords.txt')
    
    for split in ['train', 'dev', 'test']:
        df[split] = preprocess_dataframe(df[split], 'sentence', stopwords)
        print(f"  âœ… {split.capitalize()}: HoÃ n thÃ nh")
    
    # 4. Táº¡o model vÃ  train
    print(f"\nğŸ¤– Äang train model ({model_type.upper()})...")
    pipeline = create_pipeline(model_type)
    
    X_train = df['train']['sentence_processed']
    y_train = df['train']['sentiment_encoded']
    X_dev = df['dev']['sentence_processed']
    y_dev = df['dev']['sentiment_encoded']
    X_test = df['test']['sentence_processed']
    y_test = df['test']['sentiment_encoded']
    
    pipeline.fit(X_train, y_train)
    print("  âœ… Training hoÃ n thÃ nh!")
    
    # 5. TÃ¬m optimal threshold
    print("\nğŸ¯ Äang tÃ¬m optimal threshold...")
    optimal_threshold = find_optimal_threshold(pipeline, X_dev, y_dev)
    print(f"  âœ… Optimal threshold: {optimal_threshold:.4f}")
    
    # 6. ÄÃ¡nh giÃ¡
    print("\nğŸ“Š ÄÃNH GIÃ MODEL")
    print("-" * 60)
    
    for split_name, X, y in [('Train', X_train, y_train),
                              ('Dev', X_dev, y_dev),
                              ('Test', X_test, y_test)]:
        results = evaluate_model(pipeline, X, y, optimal_threshold)
        print(f"{split_name:6s} | Accuracy: {results['accuracy']:.4f} | F1-Score: {results['f1_score']:.4f}")
    
    # 7. LÆ°u model
    print(f"\nğŸ’¾ Äang lÆ°u model vÃ o {output_dir}/...")
    
    # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    # LÆ°u cÃ¡c file
    joblib.dump(pipeline, f'{output_dir}/sentiment_pipeline.pkl')
    joblib.dump(label_encoder, f'{output_dir}/label_encoder.pkl')
    joblib.dump(stopwords, f'{output_dir}/stopwords.pkl')
    
    # LÆ°u metadata
    metadata = {
        'model_type': model_type,
        'model_name': 'Linear SVM' if model_type == 'svm' else 'Logistic Regression' if model_type == 'lr' else 'Naive Bayes',
        'optimal_threshold': optimal_threshold,
        'f1_score': evaluate_model(pipeline, X_test, y_test, optimal_threshold)['f1_score'],
        'accuracy': evaluate_model(pipeline, X_test, y_test, optimal_threshold)['accuracy']
    }
    joblib.dump(metadata, f'{output_dir}/model_metadata.pkl')
    
    print("  âœ… sentiment_pipeline.pkl")
    print("  âœ… label_encoder.pkl")
    print("  âœ… stopwords.pkl")
    print("  âœ… model_metadata.pkl")
    
    print("\n" + "="*60)
    print("âœ¨ HOÃ€N THÃ€NH!")
    print("="*60)
    
    return pipeline, label_encoder, metadata


if __name__ == "__main__":
    # Train model
    pipeline, label_encoder, metadata = train_and_save_model(
        data_path='../archive',
        output_dir='models',
        model_type='svm'  # CÃ³ thá»ƒ thay báº±ng 'lr' hoáº·c 'nb'
    )
    
    print("\nâœ… CÃ³ thá»ƒ cháº¡y demo báº±ng: streamlit run streamlit_app.py")

